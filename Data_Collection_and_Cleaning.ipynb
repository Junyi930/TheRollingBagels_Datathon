{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Combine three comments file into an integrated comments file"
      ],
      "metadata": {
        "id": "SXv8To-lLNhX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5Q7-rqmI6G5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20d93173-b178-4b50-9494-44c250812f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Integrated file saved at: C:\\Users\\User\\Desktop\\Datathon\\comments_integrated.csv\n",
            "Total rows after cleaning: 2999998\n",
            "Duplicates removed: 0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: List your three comment files manually\n",
        "files = [\n",
        "    r\"C:\\Users\\User\\Desktop\\Datathon\\Dataset-20250902T161158Z-1-001\\Dataset\\dataset\\comments1.csv\",\n",
        "    r\"C:\\Users\\User\\Desktop\\Datathon\\Dataset-20250902T161158Z-1-001\\Dataset\\dataset\\comments2.csv\",\n",
        "    r\"C:\\Users\\User\\Desktop\\Datathon\\Dataset-20250902T161158Z-1-001\\Dataset\\dataset\\comments3.csv\"\n",
        "]\n",
        "\n",
        "# Step 2: Read and combine\n",
        "df_list = [pd.read_csv(f) for f in files]\n",
        "combined = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Step 3: Remove duplicates (optional, based on commentId)\n",
        "if \"commentId\" in combined.columns:\n",
        "    total_before = combined.shape[0]\n",
        "    combined = combined.drop_duplicates(subset=[\"commentId\"])\n",
        "    total_after = combined.shape[0]\n",
        "    duplicates_removed = total_before - total_after\n",
        "else:\n",
        "    duplicates_removed = 0\n",
        "    total_after = combined.shape[0]\n",
        "\n",
        "# Step 4: Save as integrated file\n",
        "output_path = r\"C:\\Users\\User\\Desktop\\Datathon\\comments_integrated.csv\"\n",
        "combined.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "print(\"✅ Integrated file saved at:\", output_path)\n",
        "print(\"Total rows after cleaning:\", total_after)\n",
        "print(\"Duplicates removed:\", duplicates_removed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merge comments and videos file using inner join (videoid)"
      ],
      "metadata": {
        "id": "AwEf8o-6LWQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ====== 1) Replace these with your local file paths ======\n",
        "comments_path = r\"C:\\Users\\User\\Desktop\\Datathon\\comments_integrated.csv\"\n",
        "videos_path   = r\"C:\\Users\\User\\Desktop\\Datathon\\Dataset-20250902T161158Z-1-001\\Dataset\\dataset\\videos.csv\"\n",
        "output_path   = r\"C:\\Users\\User\\Desktop\\Datathon\\comments_videos_integrated.csv\"\n",
        "\n",
        "# ====== 2) Load files ======\n",
        "comments = pd.read_csv(comments_path)\n",
        "videos   = pd.read_csv(videos_path)\n",
        "\n",
        "# Defensive: ensure videoId exists\n",
        "if \"videoId\" not in comments.columns or \"videoId\" not in videos.columns:\n",
        "    raise ValueError(\"Both files must contain a 'videoId' column.\")\n",
        "\n",
        "# ====== 3) Rename columns as requested ======\n",
        "# Comments side\n",
        "# - likeCount -> comment_likeCount\n",
        "# - publishedAt -> comment_publishedAt\n",
        "# - 'kind' -> replaced by new 'comment' column with sequential labels per video\n",
        "if \"likeCount\" in comments.columns:\n",
        "    comments = comments.rename(columns={\"likeCount\": \"comment_likeCount\"})\n",
        "if \"publishedAt\" in comments.columns:\n",
        "    comments = comments.rename(columns={\"publishedAt\": \"comment_publishedAt\"})\n",
        "\n",
        "# Create sequential 'comment' labels per video (comment 1, comment 2, ...)\n",
        "# If you prefer one global sequence, replace groupby with a simple range.\n",
        "comments = comments.copy()\n",
        "comments[\"_seq\"] = comments.groupby(\"videoId\").cumcount() + 1\n",
        "comments[\"comment\"] = \"comment \" + comments[\"_seq\"].astype(str)\n",
        "# If an original 'kind' column exists, drop it; we’ve replaced it with 'comment'\n",
        "if \"kind\" in comments.columns:\n",
        "    comments = comments.drop(columns=[\"kind\"])\n",
        "comments = comments.drop(columns=[\"_seq\"])\n",
        "\n",
        "# ====== 4) Clean/drop unwanted columns on videos side ======\n",
        "# You asked to remove:\n",
        "#   \"kind\", \"channelID\", \"defaultlangusge\", \"defaultaudiolanguage\", \"favourite count\"\n",
        "# Actual columns in your videos.csv appear as:\n",
        "#   kind, channelId, defaultLanguage, defaultAudioLanguage, favouriteCount\n",
        "# We'll drop using a tolerant matcher.\n",
        "to_drop_variants = {\n",
        "    \"kind\": [\"kind\"],\n",
        "    \"channelId\": [\"channelId\", \"channelID\", \"ChannelId\", \"ChannelID\"],\n",
        "    \"defaultLanguage\": [\"defaultLanguage\", \"defaultlangusge\", \"defaultLangusge\"],\n",
        "    \"defaultAudioLanguage\": [\"defaultAudioLanguage\", \"defaultaudiolanguage\"],\n",
        "    \"favouriteCount\": [\"favouriteCount\", \"favourite count\", \"favoriteCount\", \"favorite count\"]\n",
        "}\n",
        "drop_cols = []\n",
        "for canonical, variants in to_drop_variants.items():\n",
        "    for v in variants:\n",
        "        if v in videos.columns:\n",
        "            drop_cols.append(v)\n",
        "# Drop duplicates in the drop list, then drop from df\n",
        "drop_cols = list(dict.fromkeys(drop_cols))\n",
        "videos = videos.drop(columns=[c for c in drop_cols if c in videos.columns], errors=\"ignore\")\n",
        "\n",
        "# ====== 5) Rename videos columns as requested ======\n",
        "# - likeCount -> video_likeCount\n",
        "# - publishedAt -> video_publishedAt\n",
        "if \"likeCount\" in videos.columns:\n",
        "    videos = videos.rename(columns={\"likeCount\": \"video_likeCount\"})\n",
        "if \"publishedAt\" in videos.columns:\n",
        "    videos = videos.rename(columns={\"publishedAt\": \"video_publishedAt\"})\n",
        "\n",
        "# ====== 6) Count eliminations BEFORE inner join ======\n",
        "comments_rows_before = comments.shape[0]\n",
        "videos_rows_before   = videos.shape[0]\n",
        "\n",
        "# Rows eliminated due to NO matching videoId (pre-compute per-file eliminations)\n",
        "vid_ids_in_videos   = set(videos[\"videoId\"].astype(str))\n",
        "vid_ids_in_comments = set(comments[\"videoId\"].astype(str))\n",
        "\n",
        "comments_eliminated = comments[~comments[\"videoId\"].astype(str).isin(vid_ids_in_videos)].shape[0]\n",
        "videos_eliminated   = videos[~videos[\"videoId\"].astype(str).isin(vid_ids_in_comments)].shape[0]\n",
        "\n",
        "# ====== 7) Inner join on videoId ======\n",
        "merged = pd.merge(\n",
        "    comments,\n",
        "    videos,\n",
        "    on=\"videoId\",\n",
        "    how=\"inner\",\n",
        "    suffixes=(\"\", \"_video\")  # we already renamed key overlaps\n",
        ")\n",
        "\n",
        "# ====== 8) Save + report ======\n",
        "merged.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"✅ Merge complete.\")\n",
        "print(f\"Output saved to: {output_path}\")\n",
        "print(f\"Final merged shape: {merged.shape[0]} rows × {merged.shape[1]} cols\")\n",
        "\n",
        "print(\"\\n🧮 Rows eliminated due to no matching videoId (prior to inner join):\")\n",
        "print(f\"- From comments: {comments_eliminated} row(s) (out of {comments_rows_before})\")\n",
        "print(f\"- From videos:   {videos_eliminated} row(s) (out of {videos_rows_before})\")\n",
        "\n",
        "# Optional: quick peek at columns\n",
        "print(\"\\nColumns in merged:\")\n",
        "print(list(merged.columns))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3DL6C1JLMwI",
        "outputId": "777fdd89-bf50-46eb-ffbc-f4d3b6b5d8ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Merge complete.\n",
            "Output saved to: C:\\Users\\User\\Desktop\\Datathon\\comments_videos_integrated.csv\n",
            "Final merged shape: 1048231 rows × 19 cols\n",
            "\n",
            "🧮 Rows eliminated due to no matching videoId (prior to inner join):\n",
            "- From comments: 344 row(s) (out of 1048575)\n",
            "- From videos:   68081 row(s) (out of 92759)\n",
            "\n",
            "Columns in merged:\n",
            "['commentId', 'channelId', 'videoId', 'authorId', 'textOriginal', 'parentCommentId', 'comment_likeCount', 'comment_publishedAt', 'updatedAt', 'comment', 'video_publishedAt', 'title', 'description', 'tags', 'contentDuration', 'viewCount', 'video_likeCount', 'commentCount', 'topicCategories']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further Detail Cleaning"
      ],
      "metadata": {
        "id": "8TMFU0yefRtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "import sys\n",
        "!pip install ftfy emoji\n",
        "\n",
        "# =========================\n",
        "# 1) REQUIRED DEPENDENCIES\n",
        "# =========================\n",
        "# ftfy: robust Unicode repair (fixes mojibake like \"Ã¢ÂÂ¤\" -> \"❤\")\n",
        "try:\n",
        "    from ftfy import fix_text\n",
        "except Exception:\n",
        "    raise ImportError(\"ftfy is required. Install with: pip install ftfy\")\n",
        "\n",
        "# emoji: convert ALL emojis (incl. flags/ZWJ) to readable English\n",
        "try:\n",
        "    import emoji\n",
        "except Exception:\n",
        "    raise ImportError(\"emoji is required. Install with: pip install emoji\")\n",
        "\n",
        "# =========================\n",
        "# 2) PATHS (EDIT THESE)\n",
        "# =========================\n",
        "input_path  = r\"C:\\Users\\User\\Desktop\\Datathon\\comments_videos_integrated.csv\"\n",
        "output_path = r\"C:\\Users\\User\\Desktop\\Datathon\\comments_videos_cleaned.csv\"\n",
        "\n",
        "# =========================\n",
        "# 3) HELPERS\n",
        "# =========================\n",
        "URL_SAFE_CHARS = set(\"@:/?=.&%+-_\")  # keep URL/handle punctuation so links/mentions survive\n",
        "\n",
        "# Heuristic markers that often appear in mojibake\n",
        "_MOJI_MARKERS_RE = re.compile(r\"[ÃÂâð]\")\n",
        "\n",
        "def robust_fix_mojibake(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Repair broken Unicode like 'Ã¢ÂÂ¤' -> '❤'\n",
        "    Strategy:\n",
        "      1) ftfy.fix_text()\n",
        "      2) If tell-tale markers remain, try latin-1 -> utf-8\n",
        "      3) If still present, try cp1252 -> utf-8\n",
        "      4) ftfy again\n",
        "    \"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "    t = s\n",
        "    try:\n",
        "        t = fix_text(t)\n",
        "    except Exception:\n",
        "        pass\n",
        "    if _MOJI_MARKERS_RE.search(t):\n",
        "        try:\n",
        "            t = t.encode(\"latin-1\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    if _MOJI_MARKERS_RE.search(t):\n",
        "        try:\n",
        "            t = t.encode(\"cp1252\", errors=\"ignore\").decode(\"utf-8\", errors=\"ignore\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    try:\n",
        "        t = fix_text(t)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return t\n",
        "\n",
        "def emoji_to_words_all(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert every emoji to plain English words.\n",
        "    - Uses emoji.replace_emoji when available (handles complex ZWJ/flags).\n",
        "    - Falls back to demojize -> strip colons/underscores.\n",
        "    \"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "\n",
        "    # Prefer replace_emoji (emoji>=2.0) for clean names without colons\n",
        "    try:\n",
        "        # data may be dict; 'en' provides English CLDR name (e.g., 'red_heart')\n",
        "        def _repl(ch, data):\n",
        "            if isinstance(data, dict):\n",
        "                name = data.get(\"en\", \"\")\n",
        "            else:\n",
        "                name = str(data)\n",
        "            # normalize name: underscores -> spaces, trim\n",
        "            name = name.replace(\"_\", \" \").strip()\n",
        "            return name\n",
        "        s2 = emoji.replace_emoji(s, replace=_repl)\n",
        "    except Exception:\n",
        "        # Fallback: demojize -> :shortcode: then strip colons and underscores\n",
        "        try:\n",
        "            short = emoji.demojize(s, language=\"en\")\n",
        "            s2 = re.sub(r\":([a-zA-Z0-9_]+):\", lambda m: m.group(1).replace(\"_\", \" \"), short)\n",
        "        except Exception:\n",
        "            s2 = s\n",
        "\n",
        "    # Optional: collapse double spaces that can result from replacements\n",
        "    s2 = re.sub(r\"\\s+\", \" \", s2).strip()\n",
        "    return s2\n",
        "\n",
        "def remove_unnecessary_punct_preserve_urls(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Remove punctuation EXCEPT URL/handle chars; keep letters/numbers/spaces.\n",
        "    Run this AFTER emoji_to_words_all so any colons in names won't linger.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "    out = []\n",
        "    for ch in text:\n",
        "        if unicodedata.category(ch).startswith(\"P\"):\n",
        "            if ch in URL_SAFE_CHARS:\n",
        "                out.append(ch)   # keep URL/handle punctuation\n",
        "            # else drop\n",
        "        else:\n",
        "            out.append(ch)\n",
        "    return \"\".join(out)\n",
        "\n",
        "def clean_comment_inplace(s: str) -> str:\n",
        "    \"\"\"\n",
        "    FULL CLEAN for textOriginal (in place):\n",
        "      - Mojibake repair -> real Unicode\n",
        "      - Unicode normalize (NFKC)\n",
        "      - Convert ALL emojis -> words (❤ -> 'red heart', 🇮🇳 -> 'flag India', etc.)\n",
        "      - Lowercase\n",
        "      - Remove '#' (keep hashtag words)\n",
        "      - Replace newlines/tabs with spaces\n",
        "      - Remove unnecessary punctuation (preserve URL/handle chars)\n",
        "      - Collapse spaces\n",
        "    \"\"\"\n",
        "    if not isinstance(s, str):\n",
        "        return s\n",
        "    s = robust_fix_mojibake(s)\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = emoji_to_words_all(s)   # **key step**: every emoji becomes text\n",
        "    s = s.lower()\n",
        "    s = s.replace(\"#\", \"\")\n",
        "    s = s.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
        "    s = remove_unnecessary_punct_preserve_urls(s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def clean_title_inplace(title: str) -> str:\n",
        "    \"\"\"\n",
        "    CLEAN for title (in place, lighter):\n",
        "      - Mojibake repair\n",
        "      - Unicode normalize\n",
        "      - Convert ALL emojis -> words (same as comments)\n",
        "      - Keep case (no forced lowercase)\n",
        "      - Minimal spacing tidy\n",
        "    \"\"\"\n",
        "    if not isinstance(title, str):\n",
        "        return title\n",
        "    t = robust_fix_mojibake(title)\n",
        "    t = unicodedata.normalize(\"NFKC\", t)\n",
        "    t = emoji_to_words_all(t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "# =========================\n",
        "# 4) “ONLY-NOISE” FILTERS\n",
        "# =========================\n",
        "MONTHS = r\"(jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|jul(?:y)?|aug(?:ust)?|sep(?:t|tember)?|oct(?:ober)?|nov(?:ember)?|dec(?:ember)?)\"\n",
        "ONLY_INT_DECIMAL      = re.compile(r\"^\\d+(?:\\.\\d+)?$\")\n",
        "ONLY_PERCENT          = re.compile(r\"^\\d+(?:\\.\\d+)?\\s*%$\")\n",
        "ONLY_TIME             = re.compile(r\"^\\d{1,2}:\\d{1,2}(?::\\d{1,2})?$\")\n",
        "ONLY_MONTH            = re.compile(rf\"^{MONTHS}$\", re.I)\n",
        "MONTH_DAY_1           = re.compile(rf\"^{MONTHS}\\s+\\d{{1,2}}$\", re.I)\n",
        "MONTH_DAY_2           = re.compile(rf\"^\\d{{1,2}}\\s+{MONTHS}$\", re.I)\n",
        "ONLY_WHITESPACE_EMPTY = re.compile(r\"^\\s*$\")\n",
        "\n",
        "def is_only_noise(t: object) -> bool:\n",
        "    \"\"\"\n",
        "    True if the (cleaned) string is only empty/number/percent/time/month/month+day.\n",
        "    Non-strings are treated as noise.\n",
        "    \"\"\"\n",
        "    if not isinstance(t, str):\n",
        "        return True\n",
        "    s = t.strip()\n",
        "    return (\n",
        "        bool(ONLY_WHITESPACE_EMPTY.match(s))\n",
        "        or bool(ONLY_INT_DECIMAL.match(s))\n",
        "        or bool(ONLY_PERCENT.match(s))\n",
        "        or bool(ONLY_TIME.match(s))\n",
        "        or bool(ONLY_MONTH.match(s))\n",
        "        or bool(MONTH_DAY_1.match(s))\n",
        "        or bool(MONTH_DAY_2.match(s))\n",
        "    )\n",
        "\n",
        "# =========================\n",
        "# 5) MAIN\n",
        "# =========================\n",
        "def main():\n",
        "    # Read CSV (pandas < 2.0 compatible)\n",
        "    df = pd.read_csv(input_path, encoding=\"utf-8\")\n",
        "\n",
        "    # Ensure required column exists\n",
        "    if \"textOriginal\" not in df.columns:\n",
        "        raise ValueError(\"Column 'textOriginal' not found in the input file.\")\n",
        "\n",
        "    # Clean/convert title IN PLACE (if present)\n",
        "    if \"title\" in df.columns:\n",
        "        df[\"title\"] = df[\"title\"].apply(clean_title_inplace)\n",
        "    else:\n",
        "        print(\"⚠️  Column 'title' not found; skipping title processing.\")\n",
        "\n",
        "    # Clean/convert textOriginal IN PLACE\n",
        "    df[\"textOriginal\"] = df[\"textOriginal\"].apply(clean_comment_inplace)\n",
        "\n",
        "    # Remove only-noise rows (based on cleaned textOriginal)\n",
        "    rows_before = len(df)\n",
        "    mask_noise = df[\"textOriginal\"].apply(is_only_noise)\n",
        "    removed_count = int(mask_noise.sum())\n",
        "    df = df.loc[~mask_noise].reset_index(drop=True)\n",
        "\n",
        "    # Save\n",
        "    df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    # Report\n",
        "    print(\"✅ Cleaning complete (mojibake fixed, all emojis converted to words).\")\n",
        "    print(f\"→ Saved to: {output_path}\")\n",
        "    print(f\"Rows before: {rows_before:,}\")\n",
        "    print(f\"Removed (only-number/percent/time/date-like/empty): {removed_count:,}\")\n",
        "    print(f\"Rows after:  {len(df):,}\")\n",
        "\n",
        "    # Quick preview\n",
        "    preview_cols = [\"textOriginal\"]\n",
        "    if \"title\" in df.columns:\n",
        "        preview_cols.append(\"title\")\n",
        "    print(\"\\nPreview:\")\n",
        "    try:\n",
        "        print(df[preview_cols].head(10).to_string(index=False))\n",
        "    except Exception:\n",
        "        print(df.head(10).to_string(index=False))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmt2MIhUJPNM",
        "outputId": "08db7d68-e93b-49ff-95f6-761d03e885ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from ftfy) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "   ---------------------------------------- 0.0/590.6 kB ? eta -:--:--\n",
            "   --------------------------------------- 590.6/590.6 kB 10.3 MB/s eta 0:00:00\n",
            "Installing collected packages: ftfy, emoji\n",
            "Successfully installed emoji-2.14.1 ftfy-6.3.1\n",
            "✅ Cleaning complete (mojibake fixed, all emojis converted to words).\n",
            "→ Saved to: C:\\Users\\User\\Desktop\\Datathon\\comments_videos_cleaned.csv\n",
            "Rows before: 1,048,231\n",
            "Removed (only-number/percent/time/date-like/empty): 4,396\n",
            "Rows after:  1,043,835\n",
            "\n",
            "Preview:\n",
            "                                                                                                                                                                                                                                                                                       textOriginal                                                                                                                            title\n",
            "                                                                                                                                                                                                                                                    please lesbian flag i beg you you would rock it I tried hair inspired by the PAN flag :pink heart::yellow heart::light blue heart: #pansexual #transandproud #hairtransformation\n",
            "                                                                                                                                                                                                                                                   apply mashed potato juice and mixed it with curd                                                                                5 Foundation Mistakes that Every Girl Should Know\n",
            "                                                                                                                                                                                                                                                                   69 missed calls from mars:alien:                                                                                               How To Make Small Eyes Look Bigger\n",
            "                                                                                                                                                                                                                                                                                               baaa                      20sec beauty test: BLUSH PLACEMENT for YOUR FACE! :kiss mark: #blush #kbeauty #koreanmakeup #douyin #shorts\n",
            "                                                                                                                                                                                                                                                    you look like raven from phenomena raven no cap                                                                                             BLACK GIRL TRIES KYLIE JENNER MAKEUP\n",
            "                                                                                                                                                                                                                                                                                           american                  :India:Indian Vs :United States:American Makeup Look:smiling face with heart-eyes: | #shorts | SUGAR⁩ Cosmetics\n",
            "                                                                                                                                                                                                                                                         sahi disha me ja ja raha india ka future..                                                                  Kaun kaun lipstick challenge kr skta hai ? #littleglove #shorts\n",
            "                                                                                                                                                                                     :red heart::red heart::red heart::red heart::red heart::red heart::red heart::red heart::red heart::red heart:                                                                                                     \"I look better with no hair\"\n",
            "                                                                                                                                                                                                                                      love your videos. thank you :red heart::red heart::red heart:                                                                                                                   How to bronzer\n",
            "india is the best and very beautiful :smiling face with heart-eyes::smiling face with heart-eyes::smiling face with heart-eyes::smiling face with heart-eyes::smiling face with heart-eyes::smiling face with heart-eyes::smiling face with heart-eyes::red heart::red heart::red heart::red heart:                                        India Makeup Vs England Makeup #shorts #makeuptutorial #makeuptransformation #barshapatra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === One-shot cleaner: CLEAN (not drop) punctuation in textOriginal + title; then drop empty/non-meaningful + duplicates ===\n",
        "import pandas as pd\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# ==== CONFIG ====\n",
        "INPUT_CSV  = r\"C:\\Users\\User\\Desktop\\Datathon\\comments_videos_cleaned.csv\"    # change if needed\n",
        "OUTPUT_CSV = r\"C:\\Users\\User\\Desktop\\Datathon\\comments_videos_cleaned(2).csv\"\n",
        "\n",
        "# ==== 1) Load with robust encoding ====\n",
        "def robust_read_csv(path):\n",
        "    try:\n",
        "        return pd.read_csv(path, encoding=\"utf-8\")\n",
        "    except UnicodeDecodeError:\n",
        "        return pd.read_csv(path, encoding=\"latin1\")\n",
        "\n",
        "df = robust_read_csv(INPUT_CSV)\n",
        "orig_rows = len(df)\n",
        "\n",
        "# Ensure target columns exist\n",
        "cols_to_clean = [c for c in [\"textOriginal\", \"title\"] if c in df.columns]\n",
        "if not cols_to_clean:\n",
        "    raise ValueError(\"Neither 'textOriginal' nor 'title' column found in the CSV.\")\n",
        "\n",
        "# ==== 2) Define a compact English stopword set (no downloads required) ====\n",
        "STOPWORDS = {\n",
        "    \"a\",\"about\",\"above\",\"after\",\"again\",\"against\",\"all\",\"am\",\"an\",\"and\",\"any\",\"are\",\"as\",\"at\",\n",
        "    \"be\",\"because\",\"been\",\"before\",\"being\",\"below\",\"between\",\"both\",\"but\",\"by\",\n",
        "    \"can\",\"could\",\n",
        "    \"did\",\"do\",\"does\",\"doing\",\"down\",\"during\",\n",
        "    \"each\",\"few\",\"for\",\"from\",\"further\",\n",
        "    \"had\",\"has\",\"have\",\"having\",\"he\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\n",
        "    \"i\",\"if\",\"in\",\"into\",\"is\",\"it\",\"its\",\"itself\",\n",
        "    \"just\",\n",
        "    \"me\",\"more\",\"most\",\"my\",\"myself\",\n",
        "    \"no\",\"nor\",\"not\",\"now\",\n",
        "    \"of\",\"off\",\"on\",\"once\",\"only\",\"or\",\"other\",\"our\",\"ours\",\"ourselves\",\"out\",\"over\",\"own\",\n",
        "    \"same\",\"she\",\"should\",\"so\",\"some\",\"such\",\n",
        "    \"than\",\"that\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"there\",\"these\",\"they\",\n",
        "    \"this\",\"those\",\"through\",\"to\",\"too\",\n",
        "    \"under\",\"until\",\"up\",\n",
        "    \"very\",\n",
        "    \"was\",\"we\",\"were\",\"what\",\"when\",\"where\",\"which\",\"while\",\"who\",\"whom\",\"why\",\"with\",\n",
        "    \"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\n",
        "    # Extras common in scraped text\n",
        "    \"im\",\"ive\",\"youre\",\"youve\",\"dont\",\"doesnt\",\"didnt\",\"cant\",\"wont\",\"isnt\",\"arent\",\"wasnt\",\"werent\",\n",
        "    \"hey\",\"hi\",\"ok\",\"okay\",\"yeah\",\"ya\",\"oh\",\"uh\",\"uhm\",\"um\",\"hmm\",\"lol\",\"haha\",\"hahaha\",\"amp\"\n",
        "}\n",
        "\n",
        "# ==== 3) Cleaning helpers ====\n",
        "PUNCT_NUM_NONALPHA = re.compile(r\"[^A-Za-z\\s]+\")  # keep letters and spaces only\n",
        "MULTI_SPACE = re.compile(r\"\\s+\")\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    # lower\n",
        "    s = str(s).lower()\n",
        "    # remove specific noise explicitly (covers '::', '...', unicode ellipsis, em-dashes, etc.)\n",
        "    s = s.replace(\"::\", \" \").replace(\"...\", \" \").replace(\"…\", \" \")\n",
        "    # keep only letters/spaces (remove punctuation, digits, emojis, symbols)\n",
        "    s = PUNCT_NUM_NONALPHA.sub(\" \", s)\n",
        "    # collapse spaces\n",
        "    s = MULTI_SPACE.sub(\" \", s).strip()\n",
        "    # stopword removal\n",
        "    if not s:\n",
        "        return \"\"\n",
        "    tokens = [t for t in s.split() if t not in STOPWORDS]\n",
        "    # rejoin\n",
        "    cleaned = \" \".join(tokens)\n",
        "    # final collapse (in case)\n",
        "    cleaned = MULTI_SPACE.sub(\" \", cleaned).strip()\n",
        "    return cleaned\n",
        "\n",
        "def is_meaningful(s: str) -> bool:\n",
        "    \"\"\"Consider meaningful if at least one token of length >= 2 remains.\"\"\"\n",
        "    if not s:\n",
        "        return False\n",
        "    toks = s.split()\n",
        "    return any(len(t) >= 2 for t in toks)\n",
        "\n",
        "# ==== 4) Apply cleaning to the two columns (modify in place) ====\n",
        "for c in cols_to_clean:\n",
        "    df[c] = df[c].apply(clean_text)\n",
        "\n",
        "# ==== 5) Drop rows where BOTH fields are empty or non-meaningful after cleaning ====\n",
        "meaning_mask = pd.Series(False, index=df.index)\n",
        "if all(col in df.columns for col in [\"textOriginal\",\"title\"]):\n",
        "    meaning_mask = df[\"textOriginal\"].apply(is_meaningful) | df[\"title\"].apply(is_meaningful)\n",
        "else:\n",
        "    # If only one exists, require it to be meaningful\n",
        "    only_col = cols_to_clean[0]\n",
        "    meaning_mask = df[only_col].apply(is_meaningful)\n",
        "\n",
        "dropped_empty_or_nonmeaning = int((~meaning_mask).sum())\n",
        "df = df.loc[meaning_mask].copy()\n",
        "\n",
        "# ==== 6) Drop duplicates on the cleaned textOriginal + title ====\n",
        "if all(c in df.columns for c in [\"textOriginal\",\"title\"]):\n",
        "    dup_count = int(df.duplicated(subset=[\"textOriginal\",\"title\"]).sum())\n",
        "    df = df.drop_duplicates(subset=[\"textOriginal\",\"title\"], keep=\"first\")\n",
        "else:\n",
        "    dup_count = int(df.duplicated(subset=cols_to_clean).sum())\n",
        "    df = df.drop_duplicates(subset=cols_to_clean, keep=\"first\")\n",
        "\n",
        "# ==== 7) Save and report ====\n",
        "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(\"=== Cleaning Report ===\")\n",
        "print(f\"Input file                     : {Path(INPUT_CSV).resolve()}\")\n",
        "print(f\"Output file                    : {Path(OUTPUT_CSV).resolve()}\")\n",
        "print(f\"Original rows                  : {orig_rows}\")\n",
        "print(f\"Dropped empty/non-meaning rows : {dropped_empty_or_nonmeaning}\")\n",
        "print(f\"Dropped duplicates             : {dup_count}\")\n",
        "print(f\"Final rows                     : {len(df)}\")\n",
        "print(\"\\nNotes:\")\n",
        "print(\"- Cleaned IN-ROW punctuation like '::' and '...' (did not drop rows just for having them).\")\n",
        "print(\"- Kept only alphabet characters, removed numbers/symbols/punctuation.\")\n",
        "print(\"- Removed common English stopwords to keep analyzable tokens.\")\n",
        "print(\"- Dropped rows that became empty/non-meaningful after cleaning, and removed duplicates.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yZYn1Ve3ZNl",
        "outputId": "6d1a433f-9cbe-447b-f1df-ff3a7fe1d5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Cleaning Report ===\n",
            "Input file                     : C:\\Users\\User\\Desktop\\Datathon\\comments_videos_cleaned.csv\n",
            "Output file                    : C:\\Users\\User\\Desktop\\Datathon\\comments_videos_cleaned(2).csv\n",
            "Original rows                  : 1043835\n",
            "Dropped empty/non-meaning rows : 47\n",
            "Dropped duplicates             : 154006\n",
            "Final rows                     : 889782\n",
            "\n",
            "Notes:\n",
            "- Cleaned IN-ROW punctuation like '::' and '...' (did not drop rows just for having them).\n",
            "- Kept only alphabet characters, removed numbers/symbols/punctuation.\n",
            "- Removed common English stopwords to keep analyzable tokens.\n",
            "- Dropped rows that became empty/non-meaningful after cleaning, and removed duplicates.\n"
          ]
        }
      ]
    }
  ]
}